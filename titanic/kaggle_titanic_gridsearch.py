# -*- coding: utf-8 -*-
"""kaggle_titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YyjhHdi59V73ZEknQADP-gM5mVq6VtoI
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/titanic/train.csv')
test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/titanic/test.csv')

full_data = [train, test]

test.head()

train_id = train["PassengerId"]
test_id = test["PassengerId"]

# Feature Engineering

# Title = Name
for dataset in full_data:
  dataset['Name'] = dataset['Name'].astype(str)
  dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

for dataset in full_data:
  dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',
                                               'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')
  dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
  dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
  dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')

  dataset['Title'] = dataset['Title'].map({"Mr": 0, "Master": 1, "Miss": 2, "Mrs": 3, "Others": 4})

# Sex
for dataset in full_data:
  dataset.loc[dataset['Sex'] == 'male', 'Sex'] = 0
  dataset.loc[dataset['Sex'] == 'female', 'Sex'] = 1

  
# Age
for dataset in full_data:
  dataset['Age'].fillna(dataset['Age'].mean(), inplace=True)
# train['AgeBand'] = pd.cut(train['Age'], 5)
# train['AgeBand'] = pd.qcut(train['Age'], 4)
# print(train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean())
for dataset in full_data:
  dataset.loc[dataset['Age'] <= 22, 'Age'] = 0
  dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 29), 'Age'] = 1
  dataset.loc[(dataset['Age'] > 29) & (dataset['Age'] <= 35), 'Age'] = 2
  dataset.loc[(dataset['Age'] > 35), 'Age'] = 3
# Family = Parch + SibSp
for dataset in full_data:
  dataset["Family"] = dataset["Parch"] + dataset["SibSp"]

# Fare
for dataset in full_data:
  dataset['Fare'] = dataset['Fare'].fillna(0)
  dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0
  dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
  dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2
  dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3

# Feature Selection
drop_elements = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked']
train = train.drop(drop_elements, axis=1)
test = test.drop(drop_elements, axis=1)

train_label = train['Survived']
train_data = train.drop('Survived', axis=1)

import matplotlib.pyplot as plt
import seaborn as sns

fig = plt.figure(figsize=(8,8))
sns.heatmap(train.corr(), linewidths=0.1, annot=True)
fig.savefig('heatmap.png', dpi=fig.dpi)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, make_scorer

model_list = [LogisticRegression(), 
              SVC(), 
              RandomForestClassifier(), 
              KNeighborsClassifier(), 
              GradientBoostingClassifier()]

# Parameters for each models
lg_param = {
    "C": np.logspace(-3, 3, 7), 
    "penalty": ["l1", "l2"]
}
svm_param = {
    "C": [0.001, 0.1, 10], 
    "gamma": [0.001, 1]
}
rf_param = {
    'bootstrap': [True], 
    'max_depth': [10, 30, 50], 
    'max_features': ['auto', 'sqrt'], 
    'min_samples_leaf': [1, 2, 4], 
    'min_samples_split': [2, 5, 10], 
    'n_estimators': [200, 400, 600]
}
knn_param = {
    "n_neighbors": [3, 5, 7], 
    "weights": ['uniform', 'distance'], 
    "metric": ['euclidean', 'manhattan']
}
gbm_param = {
    "loss": ["deviance"], 
    "learning_rate": [0.01, 0.05, 0.1], 
    "min_samples_leaf": np.linspace(0.1, 0.5, 3), 
    "min_samples_split": np.linspace(0.1, 0.5, 3), 
    "max_depth": [3, 5, 8], 
    "max_features": ["sqrt"], 
    "criterion": ["friedman_mse", "mae"], 
    "subsample": [0.5, 1.0], 
    "n_estimators": [10, 30, 70]
}

param_list = [lg_param, svm_param, rf_param, knn_param, gbm_param]
name_list = ["LG", "SVM", "RF", "KNN", "GBM"]

shuffle_cv = KFold(n_splits=10, shuffle=True, random_state=0)

train_acc = []
train_accum_acc = []
best_param = []

X_train, X_test, Y_train, Y_test = train_test_split(train_data, train_label, test_size=0.2, shuffle=True, random_state=0)
train_pred_accum_sum = [0] * len(X_test)
submission_df = test_id

for idx, model in enumerate(model_list):
  # Train & Predict
  grid_search = GridSearchCV(model, param_grid=param_list[idx], cv=shuffle_cv, n_jobs=1, verbose=5)
  grid_search.fit(X_train, Y_train)
  train_prediction = grid_search.predict(X_test)
  accuracy = round(accuracy_score(Y_test, train_prediction) * 100, 2)
  train_acc += [accuracy]
  best_param += [grid_search.best_params_]

  # Accumulative Accuracy
  train_pred_accum_sum = [sum(tp) for tp in list(zip(train_pred_accum_sum, train_prediction))]
  accum_accuracy = round(accuracy_score(Y_test, (np.array(train_pred_accum_sum) / (idx + 1)) > 0.5) * 100, 2)
  train_accum_acc = [accum_accuracy]

  # Test & Submit
  prediction = grid_search.predict(test)
  submission = pd.DataFrame({"PassengerId": test_id, "Survived": prediction})
  submission.to_csv("/content/drive/My Drive/Colab Notebooks/data/titanic/submission_" + name_list[idx] + ".csv", index=False)
  submission_df = pd.merge(submission_df, submission)
  print("-----" + str(idx) + " model train finished" + "-----")

print("train_accuracy: ", train_acc); print("train_accumulative_voting_accuracy: ", train_accum_acc); print("best_param: ", best_param)

